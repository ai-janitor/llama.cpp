#version 450

#extension GL_EXT_control_flow_attributes : enable
#extension GL_EXT_shader_16bit_storage : require

#extension GL_EXT_shader_explicit_arithmetic_types_int32 : require

#ifdef FLOAT16
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_subgroup_extended_types_float16 : require
#endif

#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_vote : enable

#include "types.glsl"
#include "flash_attn_base.glsl"

const uint32_t HSK_per_thread = HSK / D_split;
const uint32_t HSV_per_thread = HSV / D_split;
const uint32_t num_groups = WorkGroupSize / D_split;

const bool kv_split = num_groups > Br;
const uint32_t rows_per_thread = !kv_split ? (Br / num_groups) : 1;
const uint32_t kv_groups = kv_split ? (num_groups / Br) : 1;

layout (binding = 0) readonly buffer Q {float data_q[];};
layout (binding = 0) readonly buffer QV2 {vec2 data_qv2[];};
layout (binding = 0) readonly buffer QV4 {vec4 data_qv4[];};
layout (binding = 1) readonly buffer K {float16_t data_k[];};
layout (binding = 1) readonly buffer KV2 {f16vec2 data_kv2[];};
layout (binding = 1) readonly buffer KV4 {f16vec4 data_kv4[];};
layout (binding = 2) readonly buffer V {float16_t data_v[];};
layout (binding = 2) readonly buffer VV2 {f16vec2 data_vv2[];};
layout (binding = 2) readonly buffer VV4 {f16vec4 data_vv4[];};
layout (binding = 3) readonly buffer M {float16_t data_m[];};

shared float tmpsh[kv_split ? num_groups : 1];

const uint32_t tmpshv2_stride = (HSV / 2) + 1;
shared ACC_TYPEV2 tmpshv2[kv_split ? num_groups * tmpshv2_stride : 1];

// Store the output when doing grouped query attention.
// Rows index by Q's dimension 2, and the first N rows are valid.
void gqaStore2(const in uint32_t r, const in uint32_t c, const in ACC_TYPEV2 elems, const in uint32_t o_offset, const in uint32_t iq2, const in uint32_t N)
{
    uint32_t offset = (iq2 + r) * HSV / 2 + c;
    data_ov2[o_offset + offset] = D_TYPEV2(elems);
}

#if defined(DATA_A_F32)
FLOAT_TYPEV2 dequantize2(uint ib, uint iqs, uint a_offset, uint binding_idx) {
    if (binding_idx == BINDING_IDX_K) {
        return FLOAT_TYPEV2(k_packed2.k_data_packed[(a_offset + ib) * 2 + iqs / 2]);
    } else {
        return FLOAT_TYPEV2(v_packed2.v_data_packed[(a_offset + ib) * 2 + iqs / 2]);
    }
}
#endif

#if defined(DATA_A_Q4_0)
FLOAT_TYPEV2 dequantize2(uint ib, uint iqs, uint a_offset, uint binding_idx) {
    if (binding_idx == BINDING_IDX_K) {
        uint vui = uint(k_packed.k_data_packed16[a_offset + ib].qs[(iqs & 0xF) / 2]);
        uint shift = (iqs & 0x10) >> 2;
        vui >>= shift;

        return FLOAT_TYPE(k_packed.k_data_packed16[a_offset + ib].d) * (FLOAT_TYPEV2(vui & 0xF, (vui >> 8) & 0xF) - FLOAT_TYPE(8.0f));
    } else {
        uint vui = uint(v_packed.v_data_packed16[a_offset + ib].qs[(iqs & 0xF) / 2]);
        uint shift = (iqs & 0x10) >> 2;
        vui >>= shift;

        return FLOAT_TYPE(v_packed.v_data_packed16[a_offset + ib].d) * (FLOAT_TYPEV2(vui & 0xF, (vui >> 8) & 0xF) - FLOAT_TYPE(8.0f));
    }
}
#endif

#if defined(DATA_A_Q8_0)
FLOAT_TYPEV2 dequantize2(uint ib, uint iqs, uint a_offset, uint binding_idx) {
    if (binding_idx == BINDING_IDX_K) {
        const i8vec2 v = unpack8(int32_t(k_packed.k_data_packed16[a_offset + ib].qs[iqs / 2])).xy; // vec4 used due to #12147

        return FLOAT_TYPE(k_packed.k_data_packed16[a_offset + ib].d) * FLOAT_TYPEV2(v);
    } else {
        const i8vec2 v = unpack8(int32_t(v_packed.v_data_packed16[a_offset + ib].qs[iqs / 2])).xy; // vec4 used due to #12147

        return FLOAT_TYPE(v_packed.v_data_packed16[a_offset + ib].d) * FLOAT_TYPEV2(v);
    }
}
#endif

void main() {
#ifdef NEEDS_INIT_IQ_SHMEM
    init_iq_shmem(gl_WorkGroupSize);
#endif

    init_indices();

    const uint32_t tid = gl_LocalInvocationIndex;
    const uint32_t d_tid = gl_LocalInvocationIndex % D_split;
    const uint32_t group_id = gl_LocalInvocationIndex / D_split;
    const uint32_t kv_group_id = (num_groups > Br) ? (group_id % kv_groups) : 0;

#define tile_row(r) (!kv_split ? (group_id * rows_per_thread + (r)) : (group_id / kv_groups + (r)))

    const uint32_t q_offset = gqa_iq1*p.nb01 + (iq2*p.nb02 + iq3*p.nb03) / 4;

    FLOAT_TYPEV2 Qf[rows_per_thread][HSK_per_thread / 2];
    [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
        if (tile_row(r) < N) {
            [[unroll]] for (uint32_t d = 0; d < HSK_per_thread / 2; d++) {
                Qf[r][d] = FLOAT_TYPEV2(data_qv2[q_offset / 2 + (i * Br + tile_row(r)) * q_stride / 2 + d * D_split + d_tid] * p.scale);
            }
        }
    }

    ACC_TYPEV2 Of[rows_per_thread][HSV_per_thread / 2];
    [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            Of[r][d] = ACC_TYPEV2(0.0);
        }
    }

    float Lf[rows_per_thread], Mf[rows_per_thread];

    // Use -FLT_MAX/2 rather than -inf to reduce the possibility of NaNs, e.g. when computing Mold-M.
    const float NEG_FLT_MAX_OVER_2 = uintBitsToFloat(0xFEFFFFFF);

    [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
        Lf[r] = 0;
        Mf[r] = NEG_FLT_MAX_OVER_2;
    }

    ACC_TYPE slope[rows_per_thread];
    [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
        slope[r] = ACC_TYPE(1.0);
    }

    // ALiBi
    if (p.max_bias > 0.0f) {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            slope[r] = perElemOpComputeSlope(tile_row(r), 0, ACC_TYPE(0), iq2);
        }
    }

    const uint32_t mo_stride = CEIL_DIV(KV, 16 * Bc);
    // mo_offset will point to the tile starting at row i*Br and col 0
    uint32_t mo_offset = mo_stride * i;

#if BLOCK_SIZE > 1
    uint32_t k_offset = (ik2*p.nb12 + ik3*p.nb13) / BLOCK_BYTE_SIZE;
    uint32_t v_offset = (iv2*p.nb22 + iv3*p.nb23) / BLOCK_BYTE_SIZE;
#else
    uint32_t k_offset = (ik2*p.nb12 + ik3*p.nb13) / 2;
    uint32_t v_offset = (iv2*p.nb22 + iv3*p.nb23) / 2;
#endif
    uint32_t m_offset = gqa_iq1*KV;
    if (p.nem2 != 1 || p.nem3 != 1) {
        m_offset += ((iq3 % p.nem3) * p.nem2 + (iq2 % p.nem2)) * p.nem1 * KV;
        mo_offset += ((iq3 % p.nem3) * p.nem2 + (iq2 % p.nem2)) * CEIL_DIV(p.nem1, Br) * mo_stride;
    }

    uint32_t mask_opt = 0;
    uint32_t mask_opt_idx = ~0;
    uint32_t mask_opt_bits = 0;

    [[dont_unroll]]
    for (uint32_t j = start_j + kv_group_id; j < end_j; j += kv_groups) {
        if (MASK_ENABLE) {
            if (USE_MASK_OPT && mask_opt_idx != j / 16) {
                mask_opt_idx = j / 16;
                mask_opt = data_mask_opt[mo_offset + mask_opt_idx];
            }
            mask_opt_bits = (mask_opt >> ((j % 16) * 2)) & 0x3;
            if (mask_opt_bits == MASK_OPT_ALL_NEG_INF) {
                // skip this block
                continue;
            }
        }

        ACC_TYPE Sf[rows_per_thread];
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            Sf[r] = ACC_TYPE(0.0);
        }

        [[unroll]] for (uint32_t d = 0; d < HSK_per_thread / 2; ++d) {
            if (KV_bounds_check && j >= KV) {
                continue;
            }

            FLOAT_TYPEV2 K_Tf;
#if BLOCK_SIZE > 1
            uint coord = j * k_stride * BLOCK_SIZE + 2 * (d * D_split + d_tid);
            uint ib = coord / BLOCK_SIZE;
            uint iqs = (coord % BLOCK_SIZE);
            K_Tf = dequantize2(ib, iqs, k_offset, BINDING_IDX_K);
#else
            K_Tf = FLOAT_TYPEV2(data_kv2[k_offset / 2 + j * k_stride / 2 + d * D_split + d_tid]);
#endif
            [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                Sf[r] += ACC_TYPE(dot(Qf[r][d], K_Tf));
            }
        }

        // Compute sum across the D_split
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            if (D_split == SubGroupSize) {
                Sf[r] = subgroupAdd(Sf[r]);
            } else {
                [[unroll]] for (uint s = D_split / 2; s > 0; s >>= 1) {
                    Sf[r] += subgroupShuffleXor(Sf[r], s);
                }
            }
        }

        if (LOGIT_SOFTCAP) {
            [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                Sf[r] = ACC_TYPE(p.logit_softcap * tanh(Sf[r]));
            }
        }

        if (MASK_ENABLE && mask_opt_bits != MASK_OPT_ALL_ZERO) {
            [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                FLOAT_TYPE mvf = FLOAT_TYPE(data_m[m_offset + tile_row(r) * m_stride + j]);

                Sf[r] += slope[r]*mvf;
            }
        }

        float eMf[rows_per_thread];
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            float rowmaxf = float(Sf[r]);
            float Moldf = Mf[r];

            // M = max(rowmax, Mold)
            // P = e^(S - M)
            // eM = e^(Mold - M)
            Mf[r] = max(rowmaxf, Moldf);
            eMf[r] = exp(Moldf - Mf[r]);
            Lf[r] = eMf[r]*Lf[r];
        }

        [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
            [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                Of[r][d] = ACC_TYPE(eMf[r]) * Of[r][d];
            }
        }

        if (!KV_bounds_check || j < KV) {
            FLOAT_TYPE Pf[rows_per_thread];
            [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                Pf[r] = FLOAT_TYPE(exp(float(Sf[r]) - Mf[r]));
                Lf[r] += Pf[r];
            }

            [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                FLOAT_TYPEV2 Vf;
#if BLOCK_SIZE > 1
                uint coord = j * v_stride * BLOCK_SIZE + 2 * (d * D_split + d_tid);
                uint ib = coord / BLOCK_SIZE;
                uint iqs = (coord % BLOCK_SIZE);
                Vf = dequantize2(ib, iqs, v_offset, BINDING_IDX_V);
#else
                Vf = FLOAT_TYPEV2(data_vv2[v_offset / 2 + j * v_stride / 2 + d * D_split + d_tid]);
#endif
                [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
                    Of[r][d] += ACC_TYPEV2(Pf[r] * Vf);
                }
            }
        }
    }

    if (kv_split) {
        const uint base_group = group_id - kv_group_id;

        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            if (d_tid == 0) {
                tmpsh[group_id] = Mf[r];
            }
            barrier();

            float global_max = tmpsh[base_group];
            [[unroll]] for (uint32_t g = 1; g < kv_groups; ++g) {
                global_max = max(global_max, tmpsh[base_group + g]);
            }

            float rescale = exp(Mf[r] - global_max);
            Lf[r] *= rescale;
            [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                Of[r][d] *= ACC_TYPE(rescale);
            }

            barrier();
            if (d_tid == 0) {
                tmpsh[group_id] = Lf[r];
            }
            [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                tmpshv2[group_id * tmpshv2_stride + d * D_split + d_tid] = Of[r][d];
            }
            barrier();

            if (kv_group_id == 0) {
                [[unroll]] for (uint32_t g = 1; g < kv_groups; ++g) {
                    Lf[r] += tmpsh[base_group + g];
                    [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                        Of[r][d] += tmpshv2[(base_group + g) * tmpshv2_stride + d * D_split + d_tid];
                    }
                }
                Mf[r] = global_max;
            }
            barrier();
        }

        if (kv_group_id != 0) {
            return;
        }
    }

    // If there is split_k, then the split_k resolve shader does the final
    // division by L. Store the intermediate O value and per-row m and L values.
    if (p.k_num > 1) {
        // note: O and Q have swapped coord 1,2.
        uint32_t o_offset = HSV * p.ne1 * (split_k_index + p.k_num * (gqa_iq1 + p.ne2 * iq3)) / 2;

        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            const uint row = tile_row(r);
            if (row < N) {
                [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                    gqaStore2(row, d * D_split + d_tid, Of[r][d], o_offset, iq2, N);
                }
            }
        }

        o_offset = HSV * p.ne1 * p.k_num * p.ne2 * p.ne3 + p.ne1 * 2 * (split_k_index + p.k_num * (gqa_iq1 + p.ne2 * iq3));
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            const uint row = tile_row(r);
            if (row < N) {
                perElemOpStoreCol0(row, 0u, ACC_TYPE(Lf[r]), o_offset, iq2, N);
                perElemOpStoreCol0(row, 0u, ACC_TYPE(Mf[r]), o_offset + p.ne1, iq2, N);
            }
        }

        return;
    }

    if ((p.mask_n_head_log2 & SINK_ENABLE_BIT) != 0) {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            float sink = perElemOpGetSink(tile_row(r), 0u, ACC_TYPE(0), iq2);

            float ms = 1.0f;
            float vs = 1.0f;

            if (sink > Mf[r]) {
                ms = exp(Mf[r] - sink);

                [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                    Of[r][d] *= ACC_TYPE(ms);
                }
            } else {
                vs = exp(sink - Mf[r]);
            }

            Lf[r] = Lf[r]*ms + vs;
        }
    }

    float Lfrcp[rows_per_thread];
    [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
        Lfrcp[r] = (Lf[r] == 0.0) ? 0.0 : (1.0 / Lf[r]);
    }

    [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            Of[r][d] *= ACC_TYPE(Lfrcp[r]);
#if defined(ACC_TYPE_MAX)
            Of[r][d] = clamp(Of[r][d], -ACC_TYPE_MAX, ACC_TYPE_MAX);
#endif
        }
    }

    uint32_t o_offset = (gqa_iq1*p.ne1*HSV + iq3*p.ne2*p.ne1*HSV) / 2;

    if (p.gqa_ratio > 1) {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            const uint row = tile_row(r);
            if (row < N) {
                [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                    gqaStore2(row, d * D_split + d_tid, Of[r][d], o_offset, iq2, N);
                }
            }
        }
    } else {
        [[unroll]] for (uint32_t r = 0; r < rows_per_thread; ++r) {
            const uint row = tile_row(r);
            if (i * Br + row < N) {
                [[unroll]] for (uint32_t d = 0; d < HSV_per_thread / 2; ++d) {
                    data_ov2[o_offset + (iq2 * HSV + (i * Br + row) * p.ne1 * HSV) / 2 + d * D_split + d_tid] = D_TYPEV2(Of[r][d]);
                }
            }
        }
    }
}